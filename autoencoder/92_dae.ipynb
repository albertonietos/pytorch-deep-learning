{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0461999c44bfc2ba7c7336090f65db94",
     "grade": false,
     "grade_id": "cell-be8c5c03905df198",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 1\n",
    "<br>\n",
    "<b>Deadline:</b> May 13, 2020 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 9.2. Denoising autoencoders\n",
    "\n",
    "The goal of this exercise is to get familiar with *denoising* autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83bbf952bc63ef66213753c4d3df4ceb",
     "grade": true,
     "grade_id": "cell-4c5ad871b433468b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is ../data\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a656abba884bea225c43a917fbd0951",
     "grade": false,
     "grade_id": "cell-59bd1af0b867a73f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16d54f46952cc41603e69e73aea98e8e",
     "grade": false,
     "grade_id": "cell-b2b6a9c89bb934a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "In this exercise, we will use MNIST to create a new dataset (that we call varianceMNIST). In the new dataset, the information about the shapes of the digits is represented in the variances of the pixel intensities and not in the pixel intensities (like in MNIST). We use a custom `transform.Lambda()` to generate the dataset. Note that our dataset contains an infinite amount of samples because we generate different noise instances every time we request the data. The number of shapes is of course limited to the number of digits in the MNIST dataset.\n",
    "\n",
    "This is a challenging dataset and a plain bottleneck autoencoder (from Exercise 9.1) with a mean-squared error (MSE) loss cannot encode useful information in the bottleneck layer. However, a denoising autoencoder trained with an MSE loss is able to encode the shapes of the digits in the bottleneck layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "019586b617be8bbc28c79a50437ff101",
     "grade": false,
     "grade_id": "cell-e59e6a0054c13c82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use varianceMNIST data in this exercise\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Lambda(lambda x: x * torch.randn_like(x))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8373dd37832ca59582d6966b0bbfdfad",
     "grade": false,
     "grade_id": "cell-9e0ac40239400446",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We visualize some random training samples in the cell below. As you can see, we can quite easily identify the shapes and recognize the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84f334d2457f04f376e51df1f5770b7e",
     "grade": false,
     "grade_id": "cell-3b2f4813f230f675",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = iter(trainloader).next()\n",
    "tools.plot_images(images[:8], ncol=4, cmap=plt.cm.bwr, clim=[-3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50648356cc07337524c37315d7fb0172",
     "grade": false,
     "grade_id": "cell-64dcf0d0caa30c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Denoising autoencoder (DAE)\n",
    "\n",
    "### Optimal denoising function\n",
    "\n",
    "Suppose we corrupt an image $\\mathbf{x}$ of the varianceMNIST dataset with a zero-mean Gaussian noise with standard deviation $\\sigma_n$. For a given clean pixel value $x$, the corrupted value $\\tilde{x}$ is thus produced as:\n",
    "$$\n",
    "\\tilde{x} = x + n, \\qquad n \\sim \\mathcal{N}(0, \\sigma^2_n)\n",
    "$$\n",
    "Please do not confuse the corruption process with the generative process of the varianceMNIST dataset. We assume that the varianceMNIST dataset is given to us, while we are free to select any corruption process to train a DAE. In this experiment, we choose Gaussian corruption.\n",
    "\n",
    "Knowing the generative process of the varianceMNIST dataset (which is a bit of cheating because we usually do not know the data generative process), we can compute the optimal denoising function which produces an estimate of the clean pixel value $x$ given corrupted value $\\tilde{x}$:\n",
    "$$\n",
    "g(\\tilde{x}) = \\tilde{x} \\: \\text{sigmoid}(f(\\sigma_x^2, \\sigma_n^2))\n",
    "$$\n",
    "where $f$ is some function of the variance $\\sigma^2_x$ of a pixel intensity in the varianceMNIST dataset and the variance $\\sigma^2_n$ of the corruption noise.\n",
    "\n",
    "\n",
    "In the cell below, your task is to implement a denoising autoencoder (DAE) which can learn to approximate the optimal denoising function shown above.\n",
    "* Our DAE will be trained to learn the optimal denoising function $g(\\tilde{x})$. In each training iteration, we feed corrupted images $\\tilde{\\mathbf{x}}$ to the inputs of the DAE and provide the corresponding clean images $\\mathbf{x}$ as the targets for the DAE outputs.\n",
    "* To learn useful representations (the shapes of the digits for the varianceMNIST dataset), our DAE will have a bottleneck layer with `n_components` elements. It is the output of the encoder.\n",
    "* We are not going to use values of $\\sigma_x^2$ and $\\sigma_n^2$ inside the DAE: The value of $\\sigma_x^2$ we simply do not know. We know the value of $\\sigma_n^2$ (because we select the corruption process) but we are not going to use that value in the computations of the denoising function.\n",
    "* Look carefully at the structure of the optimal denoising function. We can select the architecture of the DAE that makes it easy to perform the computations needed for optimal denoising.\n",
    "\n",
    "The proposed architecture for the DAE:\n",
    "* Encoder:\n",
    "    * `Conv2d` layer with kernel size 5 with 6 output channels, followed by ReLU\n",
    "    * `Conv2d` layer with kernel size 5 with 16 output channels, followed by ReLU\n",
    "    * Fully-connected layer with 250 output features, followed by ReLU\n",
    "    * Fully-connected layer with `n_components`\n",
    "* Decoder:\n",
    "    * Fully-connected layer with 250 output features, followed by ReLU\n",
    "    * Fully-connected layer with 250 input features, followed by ReLU\n",
    "    * `ConvTranspose2d` layer with kernel size 5 with 16 input channels, followed by ReLU\n",
    "    * `ConvTranspose2d` layer with kernel size 5 with 6 input channels\n",
    "\n",
    "Notes:\n",
    "* The exact architecture is not tested in this notebook. The above description is not full, you need to add some missing connections using the knowledge of the form of the optimal denoising function.\n",
    "* Please use recommended convolutional layers in the encoder and the decoder. If the autoencoder consists of only fully-connected layers, the learning problem is harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The optimal denoising function is of shape\n",
    "$$\n",
    "g(\\tilde{x}) = \\tilde{x} \\: \\text{sigmoid}(f(\\sigma_x^2, \\sigma_n^2))\n",
    "$$\n",
    "Then, if we assume that $f(\\cdot)$ refers to each layer, then $\\sigma_x^2$ refers to the input to the layer from the previous layer, while $\\sigma_n^2$ refers to the input from the skip connection.\n",
    "\n",
    "\n",
    "\n",
    "If we use skip connections, we let the bottleneck layer learn high level features. This is because we can use the low level features which represent small details on the pixel level and we don't need to propagate them all the way to the bottleneck layer. \n",
    "\n",
    "For example, in the varianceMNIST data set, we have noise instances (not the corrupted ones). When apply the denoising autoencoder to this data, we first add extra noise and then we want to remove this noise from the corrupted example.\n",
    "In order to succeed in this task, we have to know the noise instances that were on the original data (e.g. this pixel was blue or red) and that is the information that we can move to the decoder via the skip connections.\n",
    "Without skip connections, we would have to encode all those details in the bottleneck layer.\n",
    "However, those (whether a pixel is red or blue) are not high level features, they don't represent the shape, they represent low level features that are probably not important for us.\n",
    "So if we want our bottleneck layer to concentrate on high level features such as the shapes, the skip connections help\n",
    "\n",
    "If we would use skip connections in the vanilla autoencoder, this would not work because we get the same input and output. So if we would have skip connections, we would easily just learn a trivial mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8a4fcd848a331c854ce2413c0fe692f",
     "grade": false,
     "grade_id": "DAE",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DAE(nn.Module):\n",
    "    def __init__(self, n_components=10):\n",
    "        # YOUR CODE HERE\n",
    "        super(DAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(16*28*28, 250)\n",
    "        self.fc2 = nn.Linear(250, n_components)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(n_components, 250)\n",
    "        self.fc4 = nn.Linear(250, 16*28*28)\n",
    "        self.convtrans1 = nn.ConvTranspose2d(16, 6, kernel_size=5, padding=2)\n",
    "        self.convtrans2 = nn.ConvTranspose2d(6, 1, kernel_size=5, padding=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (tensor): input corrupted image of shape [batch_size, 1, 28, 28]\n",
    "        Returns:\n",
    "          z (tensor): encoded image of shape [batch_size, n_components]\n",
    "          y (tensor): output of shape [batch_size, 1, 28, 28]\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # Encoder\n",
    "        x_1 = F.relu(self.conv1(x))\n",
    "        x_2 = F.relu(self.conv2(x_1))\n",
    "        x_3 = F.relu(self.fc1(x_2.view(x_2.size(0), -1)))\n",
    "        z = self.fc2(x_3)\n",
    "#         z = x * \n",
    "\n",
    "        # Decoder\n",
    "        \"\"\"\n",
    "        y_3 = F.relu(self.fc3(z)) + x_3\n",
    "        y_2 = F.relu(self.fc4(y_3)) + x_2.view(x_2.size(0), -1)\n",
    "        y_1 = F.relu(self.convtrans1(y_2.view(-1, 16, 28, 28))) + x_1\n",
    "        y = self.convtrans2(y_1)\n",
    "        \"\"\"\n",
    "        y_3 = F.relu(self.fc3(z))\n",
    "        y_2 = F.relu(self.fc4(y_3)) # + x_3))\n",
    "        y_1 = F.relu(self.convtrans1(y_2.view(-1, 16, 28, 28))) # + x_2))\n",
    "#         y = self.convtrans2(y_1 + x_1)\n",
    "        y = x * torch.sigmoid(self.convtrans2(y_1))# + x_1))\n",
    "        return z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "762ad40a5382484ccd26e888e90e3868",
     "grade": false,
     "grade_id": "cell-00fa5a667ce568cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_DAE_shapes():\n",
    "    n_components = 2\n",
    "    dae = DAE(n_components)\n",
    "\n",
    "    x = torch.randn(3, 1, 28, 28)\n",
    "    z, y = dae(x)\n",
    "    assert z.shape == torch.Size([3, n_components]), f\"Bad z.shape: {z.shape}\"\n",
    "    assert y.shape == x.shape, \"Bad y.shape: {y.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_DAE_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77dcf973d8738d2241d863fce4f7c2e8",
     "grade": false,
     "grade_id": "cell-b0025c31387e75cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train a denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d993412a5a1f5975a364f1f810e798c",
     "grade": false,
     "grade_id": "cell-78dea48b9207c439",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAE(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=12544, out_features=250, bias=True)\n",
       "  (fc2): Linear(in_features=250, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=250, bias=True)\n",
       "  (fc4): Linear(in_features=250, out_features=12544, bias=True)\n",
       "  (convtrans1): ConvTranspose2d(16, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (convtrans2): ConvTranspose2d(6, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an autoencoder\n",
    "n_components = 10\n",
    "dae = DAE(n_components)\n",
    "dae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43719bd8329f33bea79acadd186912b1",
     "grade": false,
     "grade_id": "cell-05c2281f8c2211fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below. Training proceeds similarly to the standard bottleneck autoencoder. The difference is that the encoder gets *corrupted* training images as inputs and the targets are the varianceMNIST digits without the corruption noise.\n",
    "\n",
    "The recommended hyperparameters:\n",
    "* Corruption of varianceMNIST images with **additive** Gaussian noise with zero mean and standard deivation $\\sigma_n=0.2$.\n",
    "* Adam optimizer with learning rate 0.001\n",
    "* MSE loss\n",
    "\n",
    "Hints:\n",
    "- Training usually converges fast, a couple of epochs should suffice.\n",
    "- The loss at convergence should be close to 0.009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "172280b79837ad6192e61ef225d16ea9",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    n_epochs = 2\n",
    "    iters = 1\n",
    "    train_loss = []\n",
    "    sigma_n = 0.2 # std of added Gaussian noise\n",
    "    \n",
    "    optimizer = optim.Adam(dae.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in trainloader:\n",
    "            x, _ = map(lambda x: x.to(device), batch)\n",
    "            x_tilde = x + sigma_n * torch.randn_like(x, device=device)\n",
    "\n",
    "#             fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#             ax1.imshow(x.cpu()[0][0], cmap=plt.cm.bwr, clim=[-3,3])\n",
    "#             ax2.imshow(x_tilde.cpu()[0][0], cmap=plt.cm.bwr, clim=[-3,3])\n",
    "#             tools.plot_images(\n",
    "#                 torch.cat([x_tilde.cpu()[0][0], corrupted_images.cpu()[0][0]]),\n",
    "#                 ncol=2, cmap=plt.cm.bwr, clim=[-3,3]\n",
    "#             )                        \n",
    "            optimizer.zero_grad()\n",
    "            z, y = dae(x_tilde)\n",
    "            \n",
    "            loss = criterion(y, x)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            if iters % 250 == 0:\n",
    "                print(f\"Epoch {epoch}, Iteration {iters}, train_loss {loss.item()}\")\n",
    "            iters += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "424f7e706baead7b6f31324f1eec5d52",
     "grade": false,
     "grade_id": "cell-d9871235237c49d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(dae, '9_dae.pth')\n",
    "else:\n",
    "    dae = DAE(n_components=10)\n",
    "    tools.load_model(dae, '9_dae.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d0497cba0dda7555c86c847de8b8fb1",
     "grade": false,
     "grade_id": "cell-7953c9131c274027",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize embeddings\n",
    "\n",
    "Let us visualize the latent space in the cell below. If your DAE does a good job, you should clearly see ten clusters corresponding to the ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b7340d5da797517bd400d95fc6b8066",
     "grade": false,
     "grade_id": "cell-7b912a34da7ea0ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tests.visualize_embeddings(lambda x: dae(x)[0], trainloader, n_samples=1000, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4ffe217eacbfe3c570b8ded01c07647",
     "grade": false,
     "grade_id": "cell-a12ae53cbe38abd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below, we denoise some test images using the trained DAE. If your DAE does a good job, it should remove noise from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c2a021cbb39dda0b73fdf6607ec577",
     "grade": false,
     "grade_id": "cell-9e0b910e916b4703",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_denoising(trainloader):\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images[:4].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corrupted_images = images + 0.2 * torch.randn_like(images)\n",
    "        z, reconstructions = dae(corrupted_images)\n",
    "    tools.plot_images(\n",
    "        torch.cat([corrupted_images, reconstructions]),\n",
    "        ncol=4, cmap=plt.cm.bwr, clim=[-3,3]\n",
    "    )\n",
    "\n",
    "plot_denoising(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_denoising(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "564e67c421adcc4ae45ddcbd397dae6d",
     "grade": false,
     "grade_id": "cell-4ed4c7d66516fa37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test the quality of the produced embeddings by classification\n",
    "\n",
    "We will test the quality of the produced encodings by training a simple linear regression classifier using the encoded images. If the classifier gives a reasonable accuracy, this is an evidence that we learned to represent the shapes of the digits in the bottleneck layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57772a6e6ab397a144284f09033d5741",
     "grade": false,
     "grade_id": "cell-e0d4533abca6846a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c1ba9a1e331a6bace0db5dcf15bea0d",
     "grade": false,
     "grade_id": "cell-7e181194d756063b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Encode data samples using the encoder\n",
    "def encode(dataset, dae):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        for images, labels_ in dataloader:\n",
    "            z, rec = dae(images.to(device))\n",
    "            embeddings.append(z)\n",
    "            labels.append(labels_)\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "    return embeddings, labels\n",
    "\n",
    "traincodes, trainlabels = encode(trainset, dae)  # traincodes is (60000, 10)\n",
    "testcodes, testlabels = encode(testset, dae)  # testcodes is (10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "957c1af8d354975b3d6d86b4e4fe725d",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train a simple linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', max_iter=200)\n",
    "logreg.fit(traincodes.cpu(), trainlabels.cpu())\n",
    "\n",
    "predicted_labels = logreg.predict(testcodes.cpu())  # (10000,)\n",
    "\n",
    "accuracy = np.sum(testlabels.cpu().numpy() == predicted_labels) / predicted_labels.size\n",
    "print('Accuracy with a linear classifier: %.2f%%' % (accuracy*100))\n",
    "assert accuracy > .83, \"Poor accuracy of the embeddings: classification accuracy is %.2f%%\" % (accuracy*100)\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3b1a9f1756b62c370ba87c8f496a926",
     "grade": false,
     "grade_id": "cell-49dd30e7e1be67c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "In this exercise, we trained a denoising autoencoder to encode meaningful information in the bottleneck layer. The codes produced in the bottleneck layer are only 10-dimensional but they can represent useful information present in the original $28 \\times 28 = 784$-dimensional images. You can try to use in this task a plain bottleneck autoencoder (trained without the corruption process) with MSE loss and you will see that it fails to develop useful representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
